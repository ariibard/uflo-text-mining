---
format:
  revealjs:
    echo: true
    theme: custom.scss
    code-line-numbers: true
    
embed-resources: true
revealjs-plugins:
  - codewindow
---

# Modelado de t√≥picos

::: notes
En la miner√≠a de texto, solemos tener colecciones de documentos, como entradas de blog o art√≠culos de noticias, que queremos dividir en grupos naturales para poder comprenderlos por separado. El modelado de temas es un m√©todo de clasificaci√≥n no supervisada de dichos documentos, similar a la agrupaci√≥n de datos num√©ricos, que encuentra grupos naturales de elementos incluso cuando no estamos seguros de lo que buscamos.
:::

## ¬øQu√© es?

![](imagenes/1.png)

::: notes
El Topic Modelling o Modelado de T√≥picos es un tipo de modelado estad√≠stico utilizado para revelar la estructura sem√°ntica subyacente en grandes colecciones de documentos. Consiste en agrupar autom√°ticamente palabras que tienden a aparecer con frecuencia en varios documentos, con el fin de identificar grupos de palabras que representen temas distintos (University of Pensilvannia, s/f). Su objetivo principal es extraer temas latentes que representan conceptos o √°reas de inter√©s dentro del conjunto de datos, proporcionando una comprensi√≥n m√°s profunda del contenido textual.

Una de las bases fundamentales para el modelado de t√≥picos es el Modelo de Espacio Vectorial (VSM, Vector Space Model), que ha influido en diversas t√©cnicas avanzadas de recuperaci√≥n de informaci√≥n y modelado sem√°ntico. Es un modelo algebraico, basado en la matriz t√©rmino-documento, que sirvi√≥ como punto de partida para extraer informaci√≥n sem√°ntica mediante la frecuencia de t√©rminos, aunque sin considerar el orden en que aparecen las palabras.

El VSM ha evolucionado en aplicaciones de aprendizaje autom√°tico, motores de b√∫squeda, procesamiento del lenguaje natural y traducci√≥n autom√°tica, donde medir la similitud y el contexto entre palabras, frases y documentos es esencial.

Al modelar las colecciones de documentos, no todas las palabras tienen la misma relevancia; por ello, a cada t√©rmino se le asigna un peso seg√∫n su frecuencia dentro del documento. Esta ponderaci√≥n optimiza el an√°lisis tem√°tico al distinguir t√©rminos significativos de aquellos menos relevantes. Por ejemplo, al identificar la tem√°tica de un texto, es √∫til ponderar m√°s alto los sustantivos y verbos que las preposiciones, ya que los primeros aportan m√°s informaci√≥n contextual sobre el tema.
:::

## Tipos de modelados de t√≥picos

![](imagenes/2.png)

## Latent Dirichlet Allocation (LDA)

![](imagenes/3.png)

::: notes
La asignaci√≥n de Dirichlet latente (LDA) es un m√©todo particularmente popular para ajustar un modelo de temas. Trata cada documento como una mezcla de temas, y cada tema como una mezcla de palabras. Esto permite que los documentos se superpongan en cuanto a contenido, en lugar de estar separados en grupos discretos, de forma similar al uso t√≠pico del lenguaje natural.

Lo interesante de esta manera de operativizar los temas, es que cada t√≥pico puede ser entendido como un campo sem√°ntico, un conjunto de palabras que suelen correlacionar en distintos documentos. Luego, en el momento del an√°lisis de estos resultados, buscaremos inferir un tema a partir de las palabras que m√°s contribuyen a cada t√≥pico.
:::

## ¬øC√≥mo lo usamos en R? `{topicmodels}`

![](imagenes/4.png)

::: notes
Diagrama de flujo de un an√°lisis de texto que incorpora modelado de temas. El paquete topicmodels toma una Matriz Documento-T√©rmino como entrada y genera un modelo que puede ser ordenado por tidytext, de modo que pueda manipularse y visualizarse con dplyr y ggplot2.
:::

## LDA

Se rige por dos principios

-   Cada documento es una mezcla de temas
-   Cada tema es una mezcla de palabras

::: notes
Cada documento es una mezcla de temas. Imaginamos que cada documento puede contener palabras de varios temas en proporciones espec√≠ficas. Por ejemplo, en un modelo de dos temas, podr√≠amos decir: ¬´El Documento 1 contiene un 90 % del tema A y un 10 % del tema B, mientras que el Documento 2 contiene un 30 % del tema A y un 70 % del tema B¬ª.

Cada tema es una mezcla de palabras. Por ejemplo, podr√≠amos imaginar un modelo de noticias estadounidenses con dos temas, uno para "pol√≠tica" y otro para "entretenimiento". Las palabras m√°s comunes en el tema de pol√≠tica podr√≠an ser "presidente", "Congreso" y "gobierno", mientras que en el de entretenimiento podr√≠an estar compuestas por palabras como "cine", "televisi√≥n" y "actor". Es importante destacar que las palabras pueden compartirse entre temas; una palabra como "presupuesto" podr√≠a aparecer en ambos por igual.

LDA es un m√©todo matem√°tico que permite estimar ambos simult√°neamente: encontrar la combinaci√≥n de palabras asociada a cada tema y determinar la combinaci√≥n de temas que describe cada documento. Existen varias implementaciones de este algoritmo, y exploraremos una de ellas en profundidad.
:::

# Cuentos {.smaller}

Vamos a utilizar un corpus con cuentos de 57 autores en espa√±ol creado por [Karen Palacios](https://docs.google.com/presentation/d/1ROmh6rz_8Oblh-yGTz8aypbHnaxXx2R5usZnBOdz3lk/edit?slide=id.p#slide=id.p)

::: codewindow
```{r}
library(tidyverse)
library(janitor)
cuentos <- read.csv("https://raw.githubusercontent.com/karen-pal/borges/refs/heads/master/datasets/full_corpus.csv") |> 
  clean_names() |> 
  rename(id = x)
```
:::

## Observamos {.smaller}

::: codewindow
``` r
library(highcharter)

cuentos |> 
  count(author, sort= T)|> 
  arrange(n) |>  # Para que coord_flip de ggplot se vea igual en highcharter
  hchart(
    type = "bar",  # bar = horizontal; column = vertical
    hcaes(x = author, y = n)
  ) |>
  hc_chart(backgroundColor = "transparent") |>
  hc_colors("#112446") |>
  hc_xAxis(title = list(text = "")) |>
  hc_yAxis(title = list(text = "Cantidad")) |>
  hc_add_theme(hc_theme_flat())  # Similar a theme_minimal
```
:::

## Observamos {.smaller}

::: codewindow
```{r echo = FALSE}
library(highcharter)

cuentos |> 
  count(author, sort= T)|>  # Para que coord_flip de ggplot se vea igual en highcharter
  hchart(
    type = "bar",  # bar = horizontal; column = vertical
    hcaes(x = author, y = n)
  ) |>
  hc_chart(backgroundColor = "transparent") |>
  hc_colors("#112446") |>
  hc_xAxis(title = list(text = "")) |>
  hc_yAxis(title = list(text = "Cantidad")) |>
  hc_add_theme(hc_theme_flat())  # Similar a theme_minimal
```
:::

## Recortamos

Nos vamos a quedar solo con los cuentos de Borges

::: codewindow
```{r}

borges <- cuentos |> 
  filter(str_detect(author, regex("borges", T)))
```
:::

## Preprocesamiento {.smaller}

::: codewindow
``` r

library(udpipe)
modelo_sp <- udpipe::udpipe_download_model('spanish')
modelo_sp$file_model
modelo_sp <- udpipe_load_model(file = modelo_sp$file_model)

cuentos_anotados <- udpipe_annotate(
  object = modelo_sp,
  x = borges$text,
  doc_id = borges$id,
  trace = 20
  ) %>% as.data.frame(.)  

```
:::

```{r include = FALSE}

#saveRDS(cuentos_anotados, "cuentos_anotados.rds")
cuentos_anotados <- readRDS("cuentos_anotados.rds")
```

## Preprocesamiento

Convertimos a matriz

::: codewindow
```{r}
library(tidytext)
cuentos_anotados_limpios <- cuentos_anotados  %>% 
  filter(upos=="ADJ"| upos=="VERB"| upos=="NOUN") %>% 
  select( doc_id, lemma ) %>%
  filter(!lemma %in% stopwords::stopwords(language = "es"))%>%
  count(doc_id, lemma, sort = TRUE) %>% # Armamos una freq por cada documento
  cast_dtm(doc_id, lemma, n) # Convertimos a vector

```
:::

## Topic Models en R

Podemos utilizar la LDA() funci√≥n del paquete topicmodels, configurando k = 5, para crear un modelo LDA de dos temas.

::: codewindow
```{r}
library(topicmodels)

cuentos_lda <- LDA(cuentos_anotados_limpios, k = 5, control = list(seed = 1234))
cuentos_lda
```
:::

## Beta y Gamma

-   beta: probabilidad topico x palabra;
-   gamma: probabilidad topico x documento;

::: codewindow
```{r}
cuentos_lda_beta <- tidy(cuentos_lda, matrix = "beta")
cuentos_lda_gamma <- tidy(cuentos_lda, matrix = "gamma")
```
:::

::: notes
Observe que esto ha convertido el modelo a un formato de un tema por t√©rmino por fila. Para cada combinaci√≥n, el modelo calcula la probabilidad de que ese t√©rmino se genere a partir de ese tema
:::

## Beta {.smaller}

Podemos usar dplyr `slice_max()` para encontrar los 10 t√©rminos m√°s comunes en cada tema

::: codewindow
```{r}
cuentos_lda_beta %>% # principales t√©rminos en cada t√≥pico
  group_by(topic) %>%
  top_n(15) %>%
  ungroup() %>%
  arrange(topic, -beta) %>% # vamos a mostrarlo como grafico
  ggplot(aes(x=reorder(term, (beta)),y=beta)) + 
    geom_col() +
    facet_wrap(~topic, scales = "free_y") +
  coord_flip() +
  theme_minimal()
```
:::

::: notes
Como alternativa, podr√≠amos considerar los t√©rminos que ten√≠an la¬†*mayor diferencia*¬†enŒ≤Œ≤entre el tema 1 y el tema 2. Esto se puede estimar bas√°ndose en la relaci√≥n logar√≠tmica de los dos:registro2(Œ≤2Œ≤1)registro2‚Å°(Œ≤2Œ≤1)(una relaci√≥n logar√≠tmica es √∫til porque hace que la diferencia sea sim√©trica:Œ≤2Œ≤2ser el doble de grande conduce a una relaci√≥n logar√≠tmica de 1, mientras queŒ≤1Œ≤1ser el doble de grande da como resultado -1). Para restringirlo a un conjunto de palabras especialmente relevantes, podemos filtrar por palabras relativamente comunes, como aquellas que tienen unŒ≤Œ≤mayor de 1/1000 en al menos un tema.
:::

## Eliminar palabras comunes

::: notes
üéØ ¬øCu√°l es el objetivo?
Identificar, para cada t√≥pico, las palabras m√°s caracter√≠sticas, es decir, aquellas que tienen una alta probabilidad 
ùõΩ
Œ≤ en ese t√≥pico en particular, comparadas con los otros.

üß† ¬øQu√© estamos haciendo t√©cnicamente?
Expandimos la matriz 
ùõΩ
Œ≤
La cuentos_lda_beta nos da las probabilidades de cada palabra en cada t√≥pico. Al pivotearla a formato "ancho", obtenemos una fila por palabra con una columna para cada t√≥pico.

Detectamos el t√≥pico dominante para cada palabra
Usamos max.col() para encontrar, para cada t√©rmino, el t√≥pico en el que su 
ùõΩ
Œ≤ es mayor. Esto lo guardamos como topico_max.

Pivotamos de vuelta a formato largo
As√≠ podemos trabajar f√°cilmente con ggplot, filtrando solo las combinaciones (t√©rmino, t√≥pico) donde ese t√≥pico es el dominante para esa palabra.

Filtramos los t√©rminos m√°s representativos por t√≥pico
De todos los t√©rminos que aparecen en un t√≥pico, nos quedamos con los 10 de mayor 
ùõΩ
Œ≤, lo que representa las palabras m√°s importantes para ese t√≥pico.

üß™ ¬øPor qu√© no usar solo el top_n por t√≥pico?
Porque muchas veces los t√≥picos comparten palabras comunes ("dijo", "persona", "vida", etc.) y termin√°s viendo las mismas en todos. Este enfoque asegura que mostramos lo m√°s distintivo de cada t√≥pico, no solo lo m√°s frecuente.
:::

::: codewindow
```{r}
library(tidyr)

beta_wide <- cuentos_lda_beta |> 
  mutate(topic = paste0("topic", topic)) |> 
  pivot_wider(names_from = topic, values_from = beta)

# Filtrar t√©rminos con beta > 0.001 en al menos un tema
beta_wide_filtrado <- beta_wide |> 
  filter(if_any(starts_with("topic"), ~ .x > 0.001))

cols_topics <- beta_wide_filtrado |> 
  select(starts_with("topic")) |> 
  colnames()

# Luego, hacer el mutate con max.col correctamente
beta_topico_max <- beta_wide_filtrado |> 
  mutate(topico_max = cols_topics[max.col(across(all_of(cols_topics)), ties.method = "first")])
```
:::

## Filtramos {.smaller}

::: codewindow
```{r}

beta_larga <- beta_topico_max |> 
  pivot_longer(cols = all_of(cols_topics), names_to = "topic", values_to = "beta")

beta_filtrada <- beta_larga |> 
  filter(topic == topico_max) |> 
  group_by(topic) |> 
  slice_max(order_by = beta, n = 20, with_ties = FALSE) |> 
  ungroup()

ggplot(beta_filtrada, aes(x = reorder(term, beta), y = beta, fill = topic)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free_y") +
  coord_flip() +
  theme_minimal()
```
:::

## Nombres de los t√≥picos

::: codewindow
```{r}
topicos_nombres <- rbind( 
  c(topico_max = "topic1" , nombre = "Acciones"),
  c(topico_max = "topic2" , nombre = "Tiempo y espacio"),
  c(topico_max = "topic3" , nombre = "Existencia"),
  c(topico_max = "topic4" , nombre = "Conocimiento"),
  c(topico_max = "topic5" , nombre = "Reflexiones")
) %>% as_tibble()

# Agregar nombres al gr√°fico
beta_filtrada <- beta_filtrada |> 
  left_join(topicos_nombres, by = "topico_max")|> 
  group_by(topic) |> 
  slice_max(order_by = beta, n = 20, with_ties = FALSE) |> 
  ungroup()

```
:::

## Observamos {.smaller}

::: codewindow
```{r}
ggplot(beta_filtrada, aes(x = reorder(term, beta), y = beta, fill = nombre)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ nombre, scales = "free_y") +
  coord_flip() +
  theme_minimal()

```
:::


## ¬øQu√© topico tiene cada cuento?

::: codewindow
```{r}

topicos_nombres <- tibble(
  topic = 1:5,
  nombre = c("Acciones", "Tiempo y espacio", "Existencia", "Conocimiento", "Reflexiones")
)

gamma_dominante <- cuentos_lda_gamma |>
  group_by(document) |>
  slice_max(gamma, n = 1)|> 
  left_join(topicos_nombres, by = "topic")

```
:::

## Cantidad de cuentos por t√≥pico {.smaller}

::: codewindow
```{r}
gamma_dominante |> 
  count(nombre) |> 
  ggplot(aes(x = reorder(nombre, n), y = n)) +
  geom_col(fill = "#112446") +
  coord_flip() +
  theme_minimal() +
  labs(x = "", y = "Cantidad de documentos", title = "Distribuci√≥n de cuentos por t√≥pico dominante")
```
:::

## Unimos todo

::: codewindow
```{r}
cuentos_clasificados <- gamma_dominante |> 
  select(document, topic_final = topic, nombre_topico = nombre, probabilidad = gamma) |> 
  mutate(document = as.numeric(document)) |> 
  left_join(borges |> select(id, author, text), by = c("document" = "id"))

head(cuentos_clasificados)
```
:::

## Recursos

-   Blei, David. 2012. ‚ÄúProbabilistic topic models.‚Äù Communications of the ACM 55 (4): 77. https://doi.org/10.1145/2133806.2133826.

-   [https://www.tidytextmining.com/topicmodeling](https://www.tidytextmining.com/topicmodeling)

- [https://bookdown.org/gaston_becerra/curso-intro-r](https://bookdown.org/gaston_becerra/curso-intro-r)
