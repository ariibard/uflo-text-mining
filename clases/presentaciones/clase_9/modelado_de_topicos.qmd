---
format:
  revealjs:
    echo: true
    theme: custom.scss
    code-line-numbers: true
embed-resources: true
revealjs-plugins:
  - codewindow
---

# Modelado de t√≥picos

::: notes
En la miner√≠a de texto, solemos tener colecciones de documentos, como entradas de blog o art√≠culos de noticias, que queremos dividir en grupos naturales para poder comprenderlos por separado. El modelado de temas es un m√©todo de clasificaci√≥n no supervisada de dichos documentos, similar a la agrupaci√≥n de datos num√©ricos, que encuentra grupos naturales de elementos incluso cuando no estamos seguros de lo que buscamos.
:::

## ¬øQu√© es?

![](imagenes/1.png)

::: notes
El Topic Modelling o Modelado de T√≥picos es un tipo de modelado estad√≠stico utilizado para revelar la estructura sem√°ntica subyacente en grandes colecciones de documentos. Consiste en agrupar autom√°ticamente palabras que tienden a aparecer con frecuencia en varios documentos, con el fin de identificar grupos de palabras que representen temas distintos (University of Pensilvannia, s/f). Su objetivo principal es extraer temas latentes que representan conceptos o √°reas de inter√©s dentro del conjunto de datos, proporcionando una comprensi√≥n m√°s profunda del contenido textual.

Una de las bases fundamentales para el modelado de t√≥picos es el Modelo de Espacio Vectorial (VSM, Vector Space Model), que ha influido en diversas t√©cnicas avanzadas de recuperaci√≥n de informaci√≥n y modelado sem√°ntico. Es un modelo algebraico, basado en la matriz t√©rmino-documento, que sirvi√≥ como punto de partida para extraer informaci√≥n sem√°ntica mediante la frecuencia de t√©rminos, aunque sin considerar el orden en que aparecen las palabras.

El VSM ha evolucionado en aplicaciones de aprendizaje autom√°tico, motores de b√∫squeda, procesamiento del lenguaje natural y traducci√≥n autom√°tica, donde medir la similitud y el contexto entre palabras, frases y documentos es esencial.

Al modelar las colecciones de documentos, no todas las palabras tienen la misma relevancia; por ello, a cada t√©rmino se le asigna un peso seg√∫n su frecuencia dentro del documento. Esta ponderaci√≥n optimiza el an√°lisis tem√°tico al distinguir t√©rminos significativos de aquellos menos relevantes. Por ejemplo, al identificar la tem√°tica de un texto, es √∫til ponderar m√°s alto los sustantivos y verbos que las preposiciones, ya que los primeros aportan m√°s informaci√≥n contextual sobre el tema.
:::

## Tipos de modelados de t√≥picos

![](imagenes/2.png)

::: notes
existen diferentes m√©todos de modelado de t√≥picos. Estos se pueden dividir en funci√≥n de si son probabil√≠sticos, no probabil√≠sticos o basados en embeddings y tambi√©n seg√∫n el uso de enfoques de bolsa de palabras o secuencia de palabras. Los enfoques no probabil√≠sticos son enfoques algebraicos de factorizaci√≥n matricial y surgieron a

Los modelos basados en la bolsa de palabras (Bag of Words - BOW en ingl√©s) ignoran el orden de las palabras y solo se considera la frecuencia con la que aparecen en el documento. En otras palabras, los modelos basados en secuencias de palabras consideran el orden de las palabras, capturando as√≠ las relaciones entre t√©rminos en el contexto de una frase. Esto es √∫til para analizar temas cuando el contexto de una palabra cambia seg√∫n su posici√≥n (Kherwa & Bansal, 2019)

Los modelos probabil√≠sticos incluyen m√©todos como Latent Dirichlet Allocation (LDA) y Probabilistic Latent Semantic Analysis (PLSA), que emplean principios de inferencia estad√≠stica para estimar la distribuci√≥n de temas. El LDA asume que cada documento es una combinaci√≥n de temas y cada tema es una combinaci√≥n de palabras. Este modelo utiliza inferencia bayesiana para determinar la distribuci√≥n de temas en los documentos, asignando una probabilidad a cada palabra dentro de un tema‚Äã. Similar a LDA, PLSA aplica un marco probabil√≠stico para modelar la coocurrencia de palabras y detectar temas. Dentro de este tipo de modelos tambi√©n se encuentra el Hierarchical Dirichlet Process (HDP) que extiende LDA al permitir un n√∫mero indefinido de temas, adapt√°ndose a la variabilidad del corpus sin necesidad de especificar el n√∫mero de t√≥picos de antemano.

Dentro de los modelos no probabil√≠sticos, se encuentran t√©cnicas como Latent Semantic Analysis (LSA) y Non-Negative Matrix Factorization (NMF) que utilizan t√©cnicas algebraicas para descomponer la matriz de t√©rminos y captar relaciones sem√°nticas. El LSA utiliza la descomposici√≥n en valores singulares para reducir la dimensionalidad de la matriz t√©rmino-documento. Este proceso permite representar el espacio sem√°ntico de los documentos en un espacio de menor dimensi√≥n, donde las palabras y documentos con relaciones sem√°nticas y contextuales tienden a agruparse m√°s estrechamente.

As√≠, LSA facilita la identificaci√≥n de patrones y temas latentes en grandes colecciones de texto, ya que descubre asociaciones entre t√©rminos que no son evidentes en el an√°lisis superficial de la frecuencia de palabras. En esta l√≠nea, el NMF incorpora en su modelo restricciones de no negatividad, lo que puede hacer que los resultados sean m√°s interpretables.

Por √∫ltimo, los modelos basados en embeddings, como BERTopic y TopVec2, se fundamentan en una arquitectura de redes neuronales conocida como Transformers, desarrollada en el art√≠culo "Attention is All You Need". A diferencia de enfoques anteriores que analizan las palabras de manera secuencial, los Transformers eval√∫an todas las palabras en una oraci√≥n simult√°neamente, lo que les permite captar relaciones entre palabras a larga distancia.
:::

## Latent Dirichlet Allocation (LDA)

![](imagenes/3.png)

::: notes
La asignaci√≥n de Dirichlet latente (LDA) es un m√©todo particularmente popular para ajustar un modelo de temas. Trata cada documento como una mezcla de temas, y cada tema como una mezcla de palabras. Esto permite que los documentos se superpongan en cuanto a contenido, en lugar de estar separados en grupos discretos, de forma similar al uso t√≠pico del lenguaje natural.

Lo interesante de esta manera de operativizar los temas, es que cada t√≥pico puede ser entendido como un campo sem√°ntico, un conjunto de palabras que suelen correlacionar en distintos documentos. Luego, en el momento del an√°lisis de estos resultados, buscaremos inferir un tema a partir de las palabras que m√°s contribuyen a cada t√≥pico.
:::

## ¬øC√≥mo lo usamos en R? `{topicmodels}`

![](imagenes/4.png)

::: notes
Diagrama de flujo de un an√°lisis de texto que incorpora modelado de temas. El paquete topicmodels toma una Matriz Documento-T√©rmino como entrada y genera un modelo que puede ser ordenado por tidytext, de modo que pueda manipularse y visualizarse con dplyr y ggplot2.
:::

## LDA

Se rige por dos principios

-   Cada documento es una mezcla de temas
-   Cada tema es una mezcla de palabras

::: notes
Cada documento es una mezcla de temas. Imaginamos que cada documento puede contener palabras de varios temas en proporciones espec√≠ficas. Por ejemplo, en un modelo de dos temas, podr√≠amos decir: ¬´El Documento 1 contiene un 90 % del tema A y un 10 % del tema B, mientras que el Documento 2 contiene un 30 % del tema A y un 70 % del tema B¬ª.

Cada tema es una mezcla de palabras. Por ejemplo, podr√≠amos imaginar un modelo de noticias estadounidenses con dos temas, uno para "pol√≠tica" y otro para "entretenimiento". Las palabras m√°s comunes en el tema de pol√≠tica podr√≠an ser "presidente", "Congreso" y "gobierno", mientras que en el de entretenimiento podr√≠an estar compuestas por palabras como "cine", "televisi√≥n" y "actor". Es importante destacar que las palabras pueden compartirse entre temas; una palabra como "presupuesto" podr√≠a aparecer en ambos por igual.

LDA es un m√©todo matem√°tico que permite estimar ambos simult√°neamente: encontrar la combinaci√≥n de palabras asociada a cada tema y determinar la combinaci√≥n de temas que describe cada documento. Existen varias implementaciones de este algoritmo, y exploraremos una de ellas en profundidad.
:::

# Cuentos {.smaller}

Vamos a utilizar un corpus con cuentos de 57 autores en espa√±ol creado por [Karen Palacios](https://docs.google.com/presentation/d/1ROmh6rz_8Oblh-yGTz8aypbHnaxXx2R5usZnBOdz3lk/edit?slide=id.p#slide=id.p)

::: codewindow
```{r}
library(tidyverse)
library(janitor)
cuentos <- read.csv("https://raw.githubusercontent.com/karen-pal/borges/refs/heads/master/datasets/full_corpus.csv") |> 
  clean_names() |> 
  rename(id = x)
```
:::

## Observamos {.smaller}

::: codewindow
``` r
library(highcharter)

cuentos |> 
  count(author, sort= T)|> 
  arrange(n) |>  # Para que coord_flip de ggplot se vea igual en highcharter
  hchart(
    type = "bar",  # bar = horizontal; column = vertical
    hcaes(x = author, y = n)
  ) |>
  hc_chart(backgroundColor = "transparent") |>
  hc_colors("#112446") |>
  hc_xAxis(title = list(text = "")) |>
  hc_yAxis(title = list(text = "Cantidad")) |>
  hc_add_theme(hc_theme_flat())  # Similar a theme_minimal
```
:::

## Observamos {.smaller}

```{r echo = FALSE}
library(highcharter)

cuentos |> 
  count(author, sort= T)|>  # Para que coord_flip de ggplot se vea igual en highcharter
  hchart(
    type = "bar",  # bar = horizontal; column = vertical
    hcaes(x = author, y = n)
  ) |>
  hc_chart(backgroundColor = "transparent") |>
  hc_colors("#112446") |>
  hc_xAxis(title = list(text = "")) |>
  hc_yAxis(title = list(text = "Cantidad")) |>
  hc_add_theme(hc_theme_flat())  # Similar a theme_minimal
```

## Recortamos

Nos vamos a quedar solo con los cuentos de Borges

::: codewindow
```{r}

borges <- cuentos |> 
  filter(str_detect(author, regex("borges", T)))
```
:::

## Preprocesamiento {.smaller}

::: codewindow
``` r

library(udpipe)
modelo_sp <- udpipe::udpipe_download_model('spanish')
modelo_sp$file_model
modelo_sp <- udpipe_load_model(file = modelo_sp$file_model)

cuentos_anotados <- udpipe_annotate(
  object = modelo_sp,
  x = borges$text,
  doc_id = borges$id,
  trace = 20
  ) %>% as.data.frame(.)  
```
:::

```{r include = FALSE}

#saveRDS(cuentos_anotados, "cuentos_anotados.rds")
cuentos_anotados <- readRDS("cuentos_anotados.rds")
```

## Preprocesamiento

Convertimos a matriz

::: codewindow
```{r}

library(tidytext)
cuentos_anotados_limpios <- cuentos_anotados  %>% 
  filter(upos=="ADJ"| upos=="VERB"| upos=="NOUN") %>% 
  select( doc_id, lemma ) %>%
  filter(!lemma %in% stopwords::stopwords(language = "es"))%>%
  count(doc_id, lemma, sort = TRUE) %>% # Armamos una freq por cada documento
  cast_dtm(doc_id, lemma, n) # Convertimos a vector

```
:::

## Topic Models en R

Podemos utilizar la LDA() funci√≥n del paquete topicmodels, configurando k = 5, para crear un modelo LDA de dos temas.

::: codewindow
```{r}
library(topicmodels)

cuentos_lda <- LDA(cuentos_anotados_limpios, k = 6, control = list(seed = 1234))
cuentos_lda
```
:::

## Beta y Gamma

-   beta: probabilidad topico x palabra;
-   gamma: probabilidad topico x documento;

::: codewindow
```{r}
cuentos_lda_beta <- tidy(cuentos_lda, matrix = "beta")
cuentos_lda_gamma <- tidy(cuentos_lda, matrix = "gamma")
```
:::

## Beta {.smaller}

Podemos usar dplyr `slice_max()` para encontrar los 10 t√©rminos m√°s comunes en cada tema

::: codewindow
```{r}
cuentos_lda_beta %>% # principales t√©rminos en cada t√≥pico
  group_by(topic) %>%
  top_n(15) %>%
  ungroup() %>%
  arrange(topic, -beta) %>% # vamos a mostrarlo como grafico
  ggplot(aes(x=reorder(term, (beta)),y=beta)) + 
    geom_col() +
    facet_wrap(~topic, scales = "free_y") +
  coord_flip() +
  theme_minimal()
```
:::

::: notes
Como alternativa, podr√≠amos considerar los t√©rminos que ten√≠an la¬†*mayor diferencia*¬†enŒ≤Œ≤entre el tema 1 y el tema 2. Esto se puede estimar bas√°ndose en la relaci√≥n logar√≠tmica de los dos:registro2(Œ≤2Œ≤1)registro2‚Å°(Œ≤2Œ≤1)(una relaci√≥n logar√≠tmica es √∫til porque hace que la diferencia sea sim√©trica:Œ≤2Œ≤2ser el doble de grande conduce a una relaci√≥n logar√≠tmica de 1, mientras queŒ≤1Œ≤1ser el doble de grande da como resultado -1). Para restringirlo a un conjunto de palabras especialmente relevantes, podemos filtrar por palabras relativamente comunes, como aquellas que tienen unŒ≤Œ≤mayor de 1/1000 en al menos un tema.
:::

## Eliminar palabras comunes

::: notes
üéØ ¬øCu√°l es el objetivo? Identificar, para cada t√≥pico, las palabras m√°s caracter√≠sticas, es decir, aquellas que tienen una alta probabilidad ùõΩ Œ≤ en ese t√≥pico en particular, comparadas con los otros.

üß† ¬øQu√© estamos haciendo t√©cnicamente? Expandimos la matriz ùõΩ Œ≤ La cuentos_lda_beta nos da las probabilidades de cada palabra en cada t√≥pico. Al pivotearla a formato "ancho", obtenemos una fila por palabra con una columna para cada t√≥pico.

Detectamos el t√≥pico dominante para cada palabra Usamos max.col() para encontrar, para cada t√©rmino, el t√≥pico en el que su ùõΩ Œ≤ es mayor. Esto lo guardamos como topico_max.

Pivotamos de vuelta a formato largo As√≠ podemos trabajar f√°cilmente con ggplot, filtrando solo las combinaciones (t√©rmino, t√≥pico) donde ese t√≥pico es el dominante para esa palabra.

Filtramos los t√©rminos m√°s representativos por t√≥pico De todos los t√©rminos que aparecen en un t√≥pico, nos quedamos con los 10 de mayor ùõΩ Œ≤, lo que representa las palabras m√°s importantes para ese t√≥pico.

üß™ ¬øPor qu√© no usar solo el top_n por t√≥pico? Porque muchas veces los t√≥picos comparten palabras comunes ("dijo", "persona", "vida", etc.) y termin√°s viendo las mismas en todos. Este enfoque asegura que mostramos lo m√°s distintivo de cada t√≥pico, no solo lo m√°s frecuente.
:::

::: codewindow
```{r}
library(tidyr)

beta_wide <- cuentos_lda_beta |> 
  mutate(topic = paste0("topic", topic)) |> 
  pivot_wider(names_from = topic, values_from = beta)

# Filtrar t√©rminos con beta > 0.001 en al menos un tema
beta_wide_filtrado <- beta_wide |> 
  filter(if_any(starts_with("topic"), ~ .x > 0.001))

cols_topics <- beta_wide_filtrado |> 
  select(starts_with("topic")) |> 
  colnames()

# Luego, hacer el mutate con max.col correctamente
beta_topico_max <- beta_wide_filtrado |> 
  mutate(topico_max = cols_topics[max.col(across(all_of(cols_topics)), ties.method = "first")])
```
:::

## Filtramos {.smaller}

::: codewindow
```{r}

beta_larga <- beta_topico_max |> 
  pivot_longer(cols = all_of(cols_topics), names_to = "topic", values_to = "beta")

beta_filtrada <- beta_larga |> 
  filter(topic == topico_max) |> 
  group_by(topic) |> 
  slice_max(order_by = beta, n = 20, with_ties = FALSE) |> 
  ungroup()

ggplot(beta_filtrada, aes(x = reorder(term, beta), y = beta, fill = topic)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free_y") +
  coord_flip() +
  theme_minimal()
```
:::

## Nombres de los t√≥picos

::: codewindow
```{r}

topicos_nombres <- rbind( 
  c(topico_max = "topic1" , nombre = "accion"),
  c(topico_max = "topic2" , nombre = "suenio"),
  c(topico_max = "topic3" , nombre = "onirico_horizonte"),
  c(topico_max = "topic4" , nombre = "tragedia"),
  c(topico_max = "topic5" , nombre = "existencialismo"),
  c(topico_max = "topic6" , nombre = "ficcion_artistico")
) %>% as_tibble()

# Agregar nombres al gr√°fico
beta_filtrada <- beta_filtrada |> 
  left_join(topicos_nombres, by = "topico_max")|> 
  group_by(topic) |> 
  slice_max(order_by = beta, n = 20, with_ties = FALSE) |> 
  ungroup()

```
:::

## Observamos {.smaller}

::: codewindow
```{r}
ggplot(beta_filtrada, aes(x = reorder(term, beta), y = beta, fill = nombre)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ nombre, scales = "free_y") +
  coord_flip() +
  theme_minimal()

```
:::

## ¬øQu√© topico tiene cada cuento?

::: codewindow
```{r}

topicos_nombres <- tibble(
  topic = 1:5,
  nombre = c("Acciones", "Tiempo y espacio", "Existencia", "Conocimiento", "Reflexiones")
)

gamma_dominante <- cuentos_lda_gamma |>
  group_by(document) |>
  slice_max(gamma, n = 1)|> 
  left_join(topicos_nombres, by = "topic")

```
:::

## Cantidad de cuentos por t√≥pico {.smaller}

::: codewindow
```{r}
gamma_dominante |> 
  count(nombre) |> 
  ggplot(aes(x = reorder(nombre, n), y = n)) +
  geom_col(fill = "#112446") +
  coord_flip() +
  theme_minimal() +
  labs(x = "", y = "Cantidad de documentos", title = "Distribuci√≥n de cuentos por t√≥pico dominante")
```
:::

## Unimos todo

::: codewindow
```{r}
cuentos_clasificados <- gamma_dominante |> 
  select(document, topic_final = topic, nombre_topico = nombre, probabilidad = gamma) |> 
  mutate(document = as.numeric(document)) |> 
  left_join(borges |> select(id, author, text), by = c("document" = "id"))

head(cuentos_clasificados)
```
:::

## Recursos

-   Blei, David. 2012. ‚ÄúProbabilistic topic models.‚Äù Communications of the ACM 55 (4): 77. https://doi.org/10.1145/2133806.2133826.

-   <https://www.tidytextmining.com/topicmodeling>

-   <https://bookdown.org/gaston_becerra/curso-intro-r>
